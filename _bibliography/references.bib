@article{Parnas85Software,
  author     = {Parnas, David Lorge},
  title      = {Software Aspects of Strategic Defense Systems},
  year       = {1985},
  issue_date = {Dec. 1985},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {28},
  number     = {12},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/214956.214961},
  doi        = {10.1145/214956.214961},
  abstract   = {A former member of the SDIO Panel on Computing in Support of Battle Management explains why he believes the “Star Wars” effort will not achieve its stated goals.},
  journal    = {Commun. ACM},
  month      = {dec},
  pages      = {1326–1335},
  numpages   = {10}
}
@article{Robillard04HowEffective,
  author  = {Robillard, M.P. and Coelho, W. and Murphy, G.C.},
  journal = {IEEE Transactions on Software Engineering},
  title   = {How effective developers investigate source code: an exploratory study},
  year    = {2004},
  volume  = {30},
  number  = {12},
  pages   = {889-903},
  doi     = {10.1109/TSE.2004.101}
}

@book{GabrielPatterns,
  author    = {Gabriel, Richard},
  title     = {Patterns of Software: Tales from the Software Community},
  year      = {1996},
  publisher = {Oxford Univeristy Press},
  url       = {https://www.dreamsongs.com/Files/PatternsOfSoftware.pdf}
}

@inproceedings{Ying14Selection,
  author    = {Ying, Annie T. T. and Robillard, Martin P.},
  title     = {Selection and Presentation Practices for Code Example Summarization},
  year      = {2014},
  isbn      = {9781450330565},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2635868.2635877},
  doi       = {10.1145/2635868.2635877},
  abstract  = {Code examples are an important source for answering questions about software libraries and applications. Many usage contexts for code examples require them to be distilled to their essence: e.g., when serving as cues to longer documents, or for reminding developers of a previously known idiom. We conducted a study to discover how code can be summarized and why. As part of the study, we collected 156 pairs of code examples and their summaries from 16 participants, along with over 26 hours of think-aloud verbalizations detailing the decisions of the participants during their summarization activities. Based on a qualitative analysis of this data we elicited a list of practices followed by the participants to summarize code examples and propose empirically-supported hypotheses justifying the use of specific practices. One main finding was that none of the participants exclusively extracted code verbatim for the summaries, motivating abstractive summarization. The results provide a grounded basis for the development of code example summarization and presentation technology.},
  booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
  pages     = {460–471},
  numpages  = {12},
  location  = {Hong Kong, China},
  series    = {FSE 2014}
}

@inproceedings{Just14AreMutants,
  author    = {Just, Ren\'{e} and Jalali, Darioush and Inozemtseva, Laura and Ernst, Michael D. and Holmes, Reid and Fraser, Gordon},
  title     = {Are Mutants a Valid Substitute for Real Faults in Software Testing?},
  year      = {2014},
  isbn      = {9781450330565},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2635868.2635929},
  doi       = {10.1145/2635868.2635929},
  abstract  = {A good test suite is one that detects real faults. Because the set of faults in a program is usually unknowable, this definition is not useful to practitioners who are creating test suites, nor to researchers who are creating and evaluating tools that generate test suites. In place of real faults, testing research often uses mutants, which are artificial faults -- each one a simple syntactic variation -- that are systematically seeded throughout the program under test. Mutation analysis is appealing because large numbers of mutants can be automatically-generated and used to compensate for low quantities or the absence of known real faults. Unfortunately, there is little experimental evidence to support the use of mutants as a replacement for real faults. This paper investigates whether mutants are indeed a valid substitute for real faults, i.e., whether a test suite’s ability to detect mutants is correlated with its ability to detect real faults that developers have fixed. Unlike prior studies, these investigations also explicitly consider the conflating effects of code coverage on the mutant detection rate. Our experiments used 357 real faults in 5 open-source applications that comprise a total of 321,000 lines of code. Furthermore, our experiments used both developer-written and automatically-generated test suites. The results show a statistically significant correlation between mutant detection and real fault detection, independently of code coverage. The results also give concrete suggestions on how to improve mutation analysis and reveal some inherent limitations.},
  booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
  pages     = {654–665},
  numpages  = {12},
  keywords  = {real faults, Test effectiveness, code coverage, mutation analysis},
  location  = {Hong Kong, China},
  series    = {FSE 2014}
}

@inproceedings{Ernst99Dynamically,
  author    = {Ernst, Michael D. and Cockrell, Jake and Griswold, William G. and Notkin, David},
  title     = {Dynamically Discovering Likely Program Invariants to Support Program Evolution},
  year      = {1999},
  isbn      = {1581130740},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/302405.302467},
  doi       = {10.1145/302405.302467},
  booktitle = {Proceedings of the 21st International Conference on Software Engineering},
  pages     = {213–224},
  numpages  = {12},
  keywords  = {logical inference, formal specification, execution traces, dynamic analysis, program invariants, software evolution, pattern recognition},
  location  = {Los Angeles, California, USA},
  series    = {ICSE '99}
}
@article{Miller90Empirical,
  author     = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
  title      = {An Empirical Study of the Reliability of UNIX Utilities},
  year       = {1990},
  issue_date = {Dec. 1990},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {33},
  number     = {12},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/96267.96279},
  doi        = {10.1145/96267.96279},
  abstract   = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.},
  journal    = {Commun. ACM},
  month      = {dec},
  pages      = {32–44},
  numpages   = {13}
}

@article{Donaldson17Automated,
  author     = {Donaldson, Alastair F. and Evrard, Hugues and Lascu, Andrei and Thomson, Paul},
  title      = {Automated Testing of Graphics Shader Compilers},
  year       = {2017},
  issue_date = {October 2017},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {1},
  number     = {OOPSLA},
  url        = {https://doi.org/10.1145/3133917},
  doi        = {10.1145/3133917},
  abstract   = {We present an automated technique for finding defects in compilers for graphics shading languages. key challenge in compiler testing is the lack of an oracle that classifies an output as correct or incorrect; this is particularly pertinent in graphics shader compilers where the output is a rendered image that is typically under-specified. Our method builds on recent successful techniques for compiler validation based on metamorphic testing, and leverages existing high-value graphics shaders to create sets of transformed shaders that should be semantically equivalent. Rendering mismatches are then indicative of shader compilation bugs. Deviant shaders are automatically minimized to identify, in each case, a minimal change to an original high-value shader that induces a shader compiler bug. We have implemented the approach as a tool, GLFuzz, targeting the OpenGL shading language, GLSL. Our experiments over a set of 17 GPU and driver configurations, spanning the main 7 GPU designers, have led to us finding and reporting more than 60 distinct bugs, covering all tested configurations. As well as defective rendering, these issues identify security-critical vulnerabilities that affect WebGL, including a significant remote information leak security bug where a malicious web page can capture the contents of other browser tabs, and a bug whereby visiting a malicious web page can lead to a ``blue screen of death'' under Windows 10. Our findings show that shader compiler defects are prevalent, and that metamorphic testing provides an effective means for detecting them automatically.},
  journal    = {Proc. ACM Program. Lang.},
  month      = {oct},
  articleno  = {93},
  numpages   = {29},
  keywords   = {testing, compilers, shaders, OpenGL, GLSL, GPUs}
}

@inproceedings{Bohme16CoverageBased,
  author    = {B\"{o}hme, Marcel and Pham, Van-Thuan and Roychoudhury, Abhik},
  title     = {Coverage-Based Greybox Fuzzing as Markov Chain},
  year      = {2016},
  isbn      = {9781450341394},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2976749.2978428},
  doi       = {10.1145/2976749.2978428},
  abstract  = {Coverage-based Greybox Fuzzing (CGF) is a random testing approach that requires no program analysis. A new test is generated by slightly mutating a seed input. If the test exercises a new and interesting path, it is added to the set of seeds; otherwise, it is discarded. We observe that most tests exercise the same few "high-frequency" paths and develop strategies to explore significantly more paths with the same number of tests by gravitating towards low-frequency paths. We explain the challenges and opportunities of CGF using a Markov chain model which specifies the probability that fuzzing the seed that exercises path i generates an input that exercises path j. Each state (i.e., seed) has an energy that specifies the number of inputs to be generated from that seed. We show that CGF is considerably more efficient if energy is inversely proportional to the density of the stationary distribution and increases monotonically every time that seed is chosen. Energy is controlled with a power schedule.We implemented the exponential schedule by extending AFL. In 24 hours, AFLFAST exposes 3 previously unreported CVEs that are not exposed by AFL and exposes 6 previously unreported CVEs 7x faster than AFL. AFLFAST produces at least an order of magnitude more unique crashes than AFL.},
  booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {1032–1043},
  numpages  = {12},
  keywords  = {vulnerability detection, testing efficiency, software security, fuzzing, foundations},
  location  = {Vienna, Austria},
  series    = {CCS '16}
}

@inproceedings{Pacheco07Randoop,
  author    = {Pacheco, Carlos and Ernst, Michael D.},
  title     = {Randoop: Feedback-Directed Random Testing for Java},
  year      = {2007},
  isbn      = {9781595938657},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1297846.1297902},
  doi       = {10.1145/1297846.1297902},
  abstract  = {R<scp>ANDOOP</scp> for Java generates unit tests for Java code using feedback-directed random test generation. Below we describe R<scp>ANDOOP</scp>'s input, output, and test generation algorithm. We also give an overview of RANDOOP's annotation-based interface for specifying configuration parameters that affect R<scp>ANDOOP</scp>'s behavior and output.},
  booktitle = {Companion to the 22nd ACM SIGPLAN Conference on Object-Oriented Programming Systems and Applications Companion},
  pages     = {815–816},
  numpages  = {2},
  keywords  = {random testing, Java, automatic test generation},
  location  = {Montreal, Quebec, Canada},
  series    = {OOPSLA '07}
}

@inproceedings{Hilton17Tradeoffs,
  author    = {Hilton, Michael and Nelson, Nicholas and Tunnell, Timothy and Marinov, Darko and Dig, Danny},
  title     = {Trade-Offs in Continuous Integration: Assurance, Security, and Flexibility},
  year      = {2017},
  isbn      = {9781450351058},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3106237.3106270},
  doi       = {10.1145/3106237.3106270},
  abstract  = {Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI being a widely used activity in software engineering, we do not know what motivates developers to use CI, and what barriers and unmet needs they face. Without such knowledge, developers make easily avoidable errors, tool builders invest in the wrong direction, and researchers miss opportunities for improving the practice of CI. We present a qualitative study of the barriers and needs developers face when using CI. We conduct semi-structured interviews with developers from different industries and development scales. We triangulate our findings by running two surveys. We find that developers face trade-offs between speed and certainty (Assurance), between better access and information security (Security), and between more configuration options and greater ease of use (Flexi- bility). We present implications of these trade-offs for developers, tool builders, and researchers.},
  booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
  pages     = {197–207},
  numpages  = {11},
  keywords  = {Continuous Integration, Automated Testing},
  location  = {Paderborn, Germany},
  series    = {ESEC/FSE 2017}
}

@inproceedings{Memon17Taming,
  author    = {Memon, Atif and Gao, Zebao and Nguyen, Bao and Dhanda, Sanjeev and Nickell, Eric and Siemborski, Rob and Micco, John},
  title     = {Taming Google-Scale Continuous Testing},
  year      = {2017},
  isbn      = {9781538627174},
  publisher = {IEEE Press},
  url       = {https://doi.org/10.1109/ICSE-SEIP.2017.16},
  doi       = {10.1109/ICSE-SEIP.2017.16},
  abstract  = {Growth in Google's code size and feature churn rate has seen increased reliance on continuous integration (CI) and testing to maintain quality. Even with enormous resources dedicated to testing, we are unable to regression test each code change individually, resulting in increased lag time between code check-ins and test result feedback to developers. We report results of a project that aims to reduce this time by: (1) controlling test workload without compromising quality, and (2) distilling test results data to inform developers, while they write code, of the impact of their latest changes on quality. We model, empirically understand, and leverage the correlations that exist between our code, test cases, developers, programming languages, and code-change and test-execution frequencies, to improve our CI and development processes. Our findings show: very few of our tests ever fail, but those that do are generally "closer" to the code they test; certain frequently modified code and certain users/tools cause more breakages; and code recently modified by multiple developers (more than 3) breaks more often.},
  booktitle = {Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Practice Track},
  pages     = {233–242},
  numpages  = {10},
  keywords  = {software testing, continuous integration, selection},
  location  = {Buenos Aires, Argentina},
  series    = {ICSE-SEIP '17}
}

@inproceedings{Zhou20HowForking,
  author    = {Zhou, Shurui and Vasilescu, Bogdan and K\"{a}stner, Christian},
  title     = {How Has Forking Changed in the Last 20 Years? A Study of Hard Forks on GitHub},
  year      = {2020},
  isbn      = {9781450371223},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3377812.3390911},
  doi       = {10.1145/3377812.3390911},
  abstract  = {The notion of forking has changed with the rise of distributed version control systems and social coding environments, like GitHub. Traditionally forking refers to splitting off an independent development branch (which we call hard forks); research on hard forks, conducted mostly in pre-GitHub days showed that hard forks were often seen critical as they may fragment a community. Today, in social coding environments, open-source developers are encouraged to fork a project in order to contribute to the community (which we call social forks), which may have also influenced perceptions and practices around hard forks. To revisit hard forks, we identify, study, and classify 15,306 hard forks on GitHub and interview 18 owners of hard forks or forked repositories. We find that, among others, hard forks often evolve out of social forks rather than being planned deliberately and that perception about hard forks have indeed changed dramatically, seeing them often as a positive non-competitive alternative to the original project.},
  booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
  pages     = {268–269},
  numpages  = {2},
  location  = {Seoul, South Korea},
  series    = {ICSE '20}
}

@inproceedings{Overney20HowNot,
  author    = {Overney, Cassandra and Meinicke, Jens and K\"{a}stner, Christian and Vasilescu, Bogdan},
  title     = {How to Not Get Rich: An Empirical Study of Donations in Open Source},
  year      = {2020},
  isbn      = {9781450371216},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3377811.3380410},
  doi       = {10.1145/3377811.3380410},
  abstract  = {Open source is ubiquitous and many projects act as critical infrastructure, yet funding and sustaining the whole ecosystem is challenging. While there are many different funding models for open source and concerted efforts through foundations, donation platforms like PayPal, Patreon, and OpenCollective are popular and low-bar platforms to raise funds for open-source development. With a mixed-method study, we investigate the emerging and largely unexplored phenomenon of donations in open source. Specifically, we quantify how commonly open-source projects ask for donations, statistically model characteristics of projects that ask for and receive donations, analyze for what the requested funds are needed and used, and assess whether the received donations achieve the intended outcomes. We find 25,885 projects asking for donations on GitHub, often to support engineering activities; however, we also find no clear evidence that donations influence the activity level of a project. In fact, we find that donations are used in a multitude of ways, raising new research questions about effective funding.},
  booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  pages     = {1209–1221},
  numpages  = {13},
  location  = {Seoul, South Korea},
  series    = {ICSE '20}
}

@inproceedings{Kalliamvakou14Promises,
  author    = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
  title     = {The Promises and Perils of Mining GitHub},
  year      = {2014},
  isbn      = {9781450328630},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2597073.2597074},
  doi       = {10.1145/2597073.2597074},
  abstract  = {With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features---namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40\% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub.},
  booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
  pages     = {92–101},
  numpages  = {10},
  keywords  = {github, Mining software repositories, code reviews, bias, git},
  location  = {Hyderabad, India},
  series    = {MSR 2014}
}

@inproceedings{Grotov22LargeScale,
  author    = {Grotov, Konstantin and Titov, Sergey and Sotnikov, Vladimir and Golubev, Yaroslav and Bryksin, Timofey},
  title     = {A Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts},
  year      = {2022},
  isbn      = {9781450393034},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3524842.3528447},
  doi       = {10.1145/3524842.3528447},
  abstract  = {In recent years, Jupyter notebooks have grown in popularity in several domains of software engineering, such as data science, machine learning, and computer science education. Their popularity has to do with their rich features for presenting and visualizing data, however, recent studies show that notebooks also share a lot of drawbacks: high number of code clones, low reproducibility, etc. In this work, we carry out a comparison between Python code written in Jupyter Notebooks and in traditional Python scripts. We compare the code from two perspectives: structural and stylistic. In the first part of the analysis, we report the difference in the number of lines, the usage of functions, as well as various complexity metrics. In the second part, we show the difference in the number of stylistic issues and provide an extensive overview of the 15 most frequent stylistic issues in the studied mediums. Overall, we demonstrate that notebooks are characterized by the lower code complexity, however, their code could be perceived as more entangled than in the scripts. As for the style, notebooks tend to have 1.4 times more stylistic issues, but at the same time, some of them are caused by specific coding practices in notebooks and should be considered as false positives. With this research, we want to pave the way to studying specific problems of notebooks that should be addressed by the development of notebook-specific tools, and provide various insights that can be useful in this regard.},
  booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
  pages     = {353–364},
  numpages  = {12},
  location  = {Pittsburgh, Pennsylvania},
  series    = {MSR '22}
}

@inproceedings{Zahan22WeakLinks,
  author    = {Zahan, Nusrat and Zimmermann, Thomas and Godefroid, Patrice and Murphy, Brendan and Maddila, Chandra and Williams, Laurie},
  title     = {What Are Weak Links in the Npm Supply Chain?},
  year      = {2022},
  isbn      = {9781450392266},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3510457.3513044},
  doi       = {10.1145/3510457.3513044},
  abstract  = {Modern software development frequently uses third-party packages, raising the concern of supply chain security attacks. Many attackers target popular package managers, like npm, and their users with supply chain attacks. In 2021 there was a 650% year-on-year growth in security attacks by exploiting Open Source Software's supply chain. Proactive approaches are needed to predict package vulnerability to high-risk supply chain attacks. The goal of this work is to help software developers and security specialists in measuring npm supply chain weak link signals to prevent future supply chain attacks by empirically studying npm package metadata.In this paper, we analyzed the metadata of 1.63 million JavaScript npm packages. We propose six signals of security weaknesses in a software supply chain, such as the presence of install scripts, maintainer accounts associated with an expired email domain, and inactive packages with inactive maintainers. One of our case studies identified 11 malicious packages from the install scripts signal. We also found 2,818 maintainer email addresses associated with expired domains, allowing an attacker to hijack 8,494 packages by taking over the npm accounts. We obtained feedback on our weak link signals through a survey responded to by 470 npm package developers. The majority of the developers supported three out of our six proposed weak link signals. The developers also indicated that they would want to be notified about weak links signals before using third-party packages. Additionally, we discussed eight new signals suggested by package developers.},
  booktitle = {Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice},
  pages     = {331–340},
  numpages  = {10},
  keywords  = {software ecosystem, weak link signal, npm, supply chain security},
  location  = {Pittsburgh, Pennsylvania},
  series    = {ICSE-SEIP '22}
}

@inproceedings{Rahman22WhySecret,
  author    = {Rahman, Md Rayhanur and Imtiaz, Nasif and Storey, Margaret-Anne and Williams, Laurie},
  title     = {Why secret detection tools are not enough: It’s not just about false positives - An industrial case study},
  year      = {2022},
  booktitle = {Empirical Software Engineering},
  url       = {https://link.springer.com/article/10.1007/s10664-021-10109-y}
}

@article{Hermans2015Detecting,
  author   = {Hermans, Felienne
              and Pinzger, Martin
              and van Deursen, Arie},
  title    = {Detecting and refactoring code smells in spreadsheet formulas},
  journal  = {Empirical Software Engineering},
  year     = {2015},
  month    = {Apr},
  day      = {01},
  volume   = {20},
  number   = {2},
  pages    = {549-575},
  abstract = {Spreadsheets are used extensively in business processes around the world and just like software, spreadsheets are changed throughout their lifetime causing understandability and maintainability issues. This paper adapts known code smells to spreadsheet formulas. To that end we present a list of metrics by which we can detect smelly formulas; a visualization technique to highlight these formulas in spreadsheets and a method to automatically suggest refactorings to resolve smells. We implemented the metrics, visualization and refactoring suggestions techniques in a prototype tool and evaluated our approach in three studies. Firstly, we analyze the EUSES spreadsheet corpus, to study the occurrence of the formula smells. Secondly, we analyze ten real life spreadsheets, and interview the spreadsheet owners about the identified smells. Finally, we generate refactoring suggestions for those ten spreadsheets and study the implications. The results of these evaluations indicate that formula smells are common, that they can reveal real errors and weaknesses in spreadsheet formulas and that in simple cases they can be refactored.},
  issn     = {1573-7616},
  doi      = {10.1007/s10664-013-9296-2},
  url      = {https://doi.org/10.1007/s10664-013-9296-2}
}

@inproceedings{Chattopadhyay20Notebooks,
  author    = {Chattopadhyay, Souti and Prasad, Ishita and Henley, Austin Z. and Sarma, Anita and Barik, Titus},
  title     = {What's Wrong with Computational Notebooks? Pain Points, Needs, and Design Opportunities},
  year      = {2020},
  isbn      = {9781450367080},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3313831.3376729},
  doi       = {10.1145/3313831.3376729},
  abstract  = {Computational notebooks - such as Azure, Databricks, and Jupyter - are a popular, interactive paradigm for data scientists to author code, analyze data, and interleave visualizations, all within a single document. Nevertheless, as data scientists incorporate more of their activities into notebooks, they encounter unexpected difficulties, or pain points, that impact their productivity and disrupt their workflow. Through a systematic, mixed-methods study using semi-structured interviews (n=20) and survey (n=156) with data scientists, we catalog nine pain points when working with notebooks. Our findings suggest that data scientists face numerous pain points throughout the entire workflow - from setting up notebooks to deploying to production - across many notebook environments. Our data scientists report essential notebook requirements, such as supporting data exploration and visualization. The results of our study inform and inspire the design of computational notebooks.},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages     = {1–12},
  numpages  = {12},
  keywords  = {computational notebooks, data science, challenges, survey, interviews, pain points},
  location  = {Honolulu, HI, USA},
  series    = {CHI '20}
}

@inproceedings{Wang21Assessing,
  author    = {Wang, Jiawei and Kuo, Tzu-yang and Li, Li and Zeller, Andreas},
  title     = {Assessing and Restoring Reproducibility of Jupyter Notebooks},
  year      = {2021},
  isbn      = {9781450367684},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3324884.3416585},
  doi       = {10.1145/3324884.3416585},
  abstract  = {Jupyter notebooks---documents that contain live code, equations, visualizations, and narrative text---now are among the most popular means to compute, present, discuss and disseminate scientific findings. In principle, Jupyter notebooks should easily allow to reproduce and extend scientific computations and their findings; but in practice, this is not the case. The individual code cells in Jupyter notebooks can be executed in any order, with identifier usages preceding their definitions and results preceding their computations. In a sample of 936 published notebooks that would be executable in principle, we found that 73% of them would not be reproducible with straightforward approaches, requiring humans to infer (and often guess) the order in which the authors created the cells.In this paper, we present an approach to (1) automatically satisfy dependencies between code cells to reconstruct possible execution orders of the cells; and (2) instrument code cells to mitigate the impact of non-reproducible statements (i.e., random functions) in Jupyter notebooks. Our Osiris prototype takes a notebook as input and outputs the possible execution schemes that reproduce the exact notebook results. In our sample, Osiris was able to reconstruct such schemes for 82.23% of all executable notebooks, which has more than three times better than the state-of-the-art; the resulting reordered code is valid program code and thus available for further testing and analysis.},
  booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
  pages     = {138–149},
  numpages  = {12},
  location  = {Virtual Event, Australia},
  series    = {ASE '20}
}

@inproceedings{Li15WhatMakes,
  author    = {Li, Paul Luo and Ko, Andrew J. and Zhu, Jiamin},
  title     = {What Makes a Great Software Engineer?},
  year      = {2015},
  isbn      = {9781479919345},
  publisher = {IEEE Press},
  abstract  = {Good software engineers are essential to the creation of good software. However, most of what we know about software-engineering expertise are vague stereotypes, such as 'excellent communicators' and 'great teammates'. The lack of specificity in our understanding hinders researchers from reasoning about them, employers from identifying them, and young engineers from becoming them. Our understanding also lacks breadth: what are all the distinguishing attributes of great engineers (technical expertise and beyond)? We took a first step in addressing these gaps by interviewing 59 experienced engineers across 13 divisions at Microsoft, uncovering 53 attributes of great engineers. We explain the attributes and examine how the most salient of these impact projects and teams. We discuss implications of this knowledge on research and the hiring and training of engineers.},
  booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
  pages     = {700–710},
  numpages  = {11},
  keywords  = {software engineers, expertise, teamwork},
  location  = {Florence, Italy},
  series    = {ICSE '15}
}

@inproceedings{Arab22Exploratory,
  author    = {Arab, Maryam and LaToza, Thomas D. and Liang, Jenny and Ko, Amy J.},
  title     = {An Exploratory Study of Sharing Strategic Programming Knowledge},
  year      = {2022},
  isbn      = {9781450391573},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3491102.3502070},
  doi       = {10.1145/3491102.3502070},
  abstract  = {In many domains, strategic knowledge is documented and shared through checklists and handbooks. In software engineering, however, developers rarely share strategic knowledge for approaching programming problems, in contrast to other artifacts and despite its importance to productivity and success. To understand barriers to sharing, we simulated a programming strategy knowledge-sharing platform, asking experienced developers to articulate a programming strategy and others to use these strategies while providing feedback. Throughout, we asked strategy authors and users to reflect on the challenges they faced. Our analysis revealed that developers could share strategic knowledge. However, they struggled in choosing a level of detail and understanding the diversity of the potential audience. While authors required substantial feedback, users struggled to give it and authors to interpret it. Our results suggest that sharing strategic knowledge differs from sharing code and raises challenging questions about how knowledge-sharing platforms should support search and feedback.},
  booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  articleno = {66},
  numpages  = {15},
  keywords  = {Knowledge sharing, Programming strategies},
  location  = {New Orleans, LA, USA},
  series    = {CHI '22}
}

@inproceedings{Imtiaz21Comparative,
  author    = {Imtiaz, Nasif and Thorn, Seaver and Williams, Laurie},
  title     = {A Comparative Study of Vulnerability Reporting by Software Composition Analysis Tools},
  year      = {2021},
  isbn      = {9781450386654},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3475716.3475769},
  doi       = {10.1145/3475716.3475769},
  abstract  = {Background: Modern software uses many third-party libraries and frameworks as dependencies. Known vulnerabilities in these dependencies are a potential security risk. Software composition analysis (SCA) tools, therefore, are being increasingly adopted by practitioners to keep track of vulnerable dependencies. Aim: The goal of this study is to understand the difference in vulnerability reporting by various SCA tools. Understanding if and how existing SCA tools differ in their analysis may help security practitioners to choose the right tooling and identify future research needs. Method: We present an in-depth case study by comparing the analysis reports of 9 industry-leading SCA tools on a large web application, OpenMRS, composed of Maven (Java) and npm (JavaScript) projects. Results: We find that the tools vary in their vulnerability reporting. The count of reported vulnerable dependencies ranges from 17 to 332 for Maven and from 32 to 239 for npm projects across the studied tools. Similarly, the count of unique known vulnerabilities reported by the tools ranges from 36 to 313 for Maven and from 45 to 234 for npm projects. Our manual analysis of the tools' results suggest that accuracy of the vulnerability database is a key differentiator for SCA tools. Conclusion: We recommend that practitioners should not rely on any single tool at the present, as that can result in missing known vulnerabilities. We point out two research directions in the SCA space: i) establishing frameworks and metrics to identify false positives for dependency vulnerabilities; and ii) building automation technologies for continuous monitoring of vulnerability data from open source package ecosystems.},
  booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
  articleno = {5},
  numpages  = {11},
  keywords  = {supply chain security, software composition analysis, dependency, vulnerability, security tools, case study},
  location  = {Bari, Italy},
  series    = {ESEM '21}
}

@inproceedings{Sejfia22Practical,
  author    = {Sejfia, Adriana and Sch\"{a}fer, Max},
  title     = {Practical Automated Detection of Malicious Npm Packages},
  year      = {2022},
  isbn      = {9781450392211},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3510003.3510104},
  doi       = {10.1145/3510003.3510104},
  abstract  = {The npm registry is one of the pillars of the JavaScript and Type-Script ecosystems, hosting over 1.7 million packages ranging from simple utility libraries to complex frameworks and entire applications. Each day, developers publish tens of thousands of updates as well as hundreds of new packages. Due to the overwhelming popularity of npm, it has become a prime target for malicious actors, who publish new packages or compromise existing packages to introduce malware that tampers with or exfiltrates sensitive data from users who install either these packages or any package that (transitively) depends on them. Defending against such attacks is essential to maintaining the integrity of the software supply chain, but the sheer volume of package updates makes comprehensive manual review infeasible. We present Amalfi, a machine-learning based approach for automatically detecting potentially malicious packages comprised of three complementary techniques. We start with classifiers trained on known examples of malicious and benign packages. If a package is flagged as malicious by a classifier, we then check whether it includes metadata about its source repository, and if so whether the package can be reproduced from its source code. Packages that are reproducible from source are not usually malicious, so this step allows us to weed out false positives. Finally, we also employ a simple textual clone-detection technique to identify copies of malicious packages that may have been missed by the classifiers, reducing the number of false negatives. Amalfi improves on the state of the art in that it is lightweight, requiring only a few seconds per package to extract features and run the classifiers, and gives good results in practice: running it on 96287 package versions published over the course of one week, we were able to identify 95 previously unknown malware samples, with a manageable number of false positives.},
  booktitle = {Proceedings of the 44th International Conference on Software Engineering},
  pages     = {1681–1692},
  numpages  = {12},
  keywords  = {malware detection, supply chain security},
  location  = {Pittsburgh, Pennsylvania},
  series    = {ICSE '22}
}

@inproceedings{Kaldor17Canopy,
  author    = {Kaldor, Jonathan and Mace, Jonathan and Bejda, Micha\l{} and Gao, Edison and Kuropatwa, Wiktor and O'Neill, Joe and Ong, Kian Win and Schaller, Bill and Shan, Pingjia and Viscomi, Brendan and Venkataraman, Vinod and Veeraraghavan, Kaushik and Song, Yee Jiun},
  title     = {Canopy: An End-to-End Performance Tracing And Analysis System},
  year      = {2017},
  isbn      = {9781450350853},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3132747.3132749},
  doi       = {10.1145/3132747.3132749},
  abstract  = {This paper presents Canopy, Facebook's end-to-end performance tracing infrastructure. Canopy records causally related performance data across the end-to-end execution path of requests, including from browsers, mobile applications, and backend services. Canopy processes traces in near real-time, derives user-specified features, and outputs to performance datasets that aggregate across billions of requests. Using Canopy, Facebook engineers can query and analyze performance data in real-time. Canopy addresses three challenges we have encountered in scaling performance analysis: supporting the range of execution and performance models used by different components of the Facebook stack; supporting interactive ad-hoc analysis of performance data; and enabling deep customization by users, from sampling traces to extracting and visualizing features. Canopy currently records and processes over 1 billion traces per day. We discuss how Canopy has evolved to apply to a wide range of scenarios, and present case studies of its use in solving various performance challenges.},
  booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
  pages     = {34–50},
  numpages  = {17},
  location  = {Shanghai, China},
  series    = {SOSP '17}
}